{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwERn0u8zzAI"
      },
      "source": [
        "# RDEIC-LFW-DSS: ResNet-based Deep Embedded Image Clustering using Local Feature Weighting and Dynamic Sample Selection Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tn6yUKix14g",
        "outputId": "4e426017-8338-429c-8480-2887ee4d883f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/clustering\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/clustering')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4yFnRNYM8_QR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aAGuIzwW5inR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, accuracy_score, homogeneity_score\n",
        "\n",
        "nmi_fun = normalized_mutual_info_score\n",
        "ari_fun = adjusted_rand_score\n",
        "# acc_fun = homogeneity_score\n",
        "\n",
        "def acc_fun (y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate clustering accuracy. Require scikit-learn installed\n",
        "\n",
        "    # Arguments\n",
        "        y: true labels, numpy.array with shape `(n_samples,)`\n",
        "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
        "\n",
        "    # Return\n",
        "        accuracy, in [0,1]\n",
        "    \"\"\"\n",
        "    y_true = y_true.astype(np.int64)\n",
        "    assert y_pred.size == y_true.size\n",
        "    D = max(y_pred.max(), y_true.max()) + 1\n",
        "    w = np.zeros((D, D), dtype=np.int64)\n",
        "    for i in range(y_pred.size):\n",
        "        w[y_pred[i], y_true[i]] += 1\n",
        "    from scipy.optimize import linear_sum_assignment as linear_assignment\n",
        "    ind = list(linear_assignment(w.max() - w))\n",
        "    acc_sum = 0\n",
        "    for k in range(len(w)):\n",
        "       acc_sum =  w[ind[0][k],ind[1][k]] + acc_sum\n",
        "    return acc_sum * 1.0 / y_pred.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o9nNkrZE5U7I"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from keras.layers import Dense, Input, Reshape\n",
        "from keras.models import Model\n",
        "from keras import callbacks\n",
        "from keras.initializers import VarianceScaling\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans,k_means\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from scipy import optimize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mnist test Dataset\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x, y) = mnist.load_data()\n",
        "\n",
        "x = x.reshape((x.shape[0], -1))\n",
        "x = np.divide(x, 255.)\n",
        "n_clusters = 10\n",
        "img_size = (28, 28)\n",
        "update_interval = 140\n",
        "batch_size = 256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMfC-F1EaMnJ",
        "outputId": "c6db234b-873c-4a6f-a099-740b934b6853"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V2yKS3Df5U7N"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "import keras.backend as K\n",
        "\n",
        "def get_model(dims, act='relu', init='glorot_uniform'):\n",
        "    n_stacks = len(dims) - 1\n",
        "    # input\n",
        "    inputs = layers.Input(shape=(dims[0],))\n",
        "    Neighbor1 =layers.Input(shape=(dims[0],))\n",
        "    Neighbor2 =layers.Input(shape=(dims[0],))\n",
        "\n",
        "    x = inputs\n",
        "    inp_Neighbor1 = Neighbor1  \n",
        "    inp_Neighbor2 = Neighbor2\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x1 = layers.Dense(dims[1], activation=act, kernel_initializer=init)\n",
        "    x = x1 (x)\n",
        "    inp_Neighbor1 = x1    (inp_Neighbor1)   \n",
        "    inp_Neighbor2 = x1    (inp_Neighbor2)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "    previous_block_activation1 = inp_Neighbor1  # Set aside residual\n",
        "    previous_block_activation2 = inp_Neighbor2  # Set aside residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for i in range(n_stacks-2):\n",
        "        x2 = layers.Dense(dims[i + 2], activation=act, kernel_initializer=init)\n",
        "        x = x2 (x)\n",
        "        inp_Neighbor1 = x2    (inp_Neighbor1)   \n",
        "        inp_Neighbor2 = x2    (inp_Neighbor2)\n",
        "        \n",
        "\n",
        "        # Project residual\n",
        "        x3 = layers.Dense(dims[i + 2], activation=act, kernel_initializer=init)\n",
        "        x4 = layers.add  # Add back residual\n",
        "\n",
        "        residual = x3     (previous_block_activation) \n",
        "        residual1 = x3    (previous_block_activation1)   \n",
        "        residual2 = x3    (previous_block_activation2)\n",
        "           \n",
        "        x = x4   ([x, residual,inp_Neighbor1, residual1,inp_Neighbor2, residual2])  # Add back residual\n",
        "        # inp_Neighbor1 = x4   ([inp_Neighbor1, residual1])  \n",
        "        # inp_Neighbor2 = x4   ([inp_Neighbor2, residual2])\n",
        "           \n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "        previous_block_activation1 = inp_Neighbor1  # Set aside residual\n",
        "        previous_block_activation2 = inp_Neighbor2  # Set aside residual\n",
        "\n",
        "    # hidden layer\n",
        "    x5 = layers.Dense(dims[-1], kernel_initializer=init)\n",
        "    encoded = x5 (x)  # hidden layer, features are extracted from here\n",
        "    encoded1 = x5   (inp_Neighbor1)   \n",
        "    encoded2 = x5   (inp_Neighbor2)\n",
        "\n",
        "    max_encoded = layers.maximum([encoded1, encoded2],name='max_neibour')\n",
        "    Final_encoded = layers.add([encoded, max_encoded],name='add_inputs')\n",
        "    x = Final_encoded\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "    for i in range(n_stacks-1, 0, -1):\n",
        "        x = Dense(dims[i], activation=act, kernel_initializer=init)(x)\n",
        "\n",
        "        # Project residual\n",
        "        # residual = previous_block_activation\n",
        "        # residual = layers.Dense(dims[i], activation=act, kernel_initializer=init)(residual)\n",
        "        # x = layers.add([x, residual])  # Add back residual\n",
        "        # previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    x = layers.Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
        "    outputs = x\n",
        "\n",
        "    # Define the model\n",
        "    Autoencoder = Model([inputs, Neighbor1, Neighbor2], outputs, name='Autoencoder')\n",
        "    Encoder = Model([inputs, Neighbor1, Neighbor2], Final_encoded, name='encoder')\n",
        "    return Autoencoder, Encoder\n",
        "\n",
        "# Free up RAM in case the model definition cells were run multiple times\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVsvQayR5U7h"
      },
      "source": [
        "## Hyper-params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8OwWZ1o5U7h",
        "outputId": "6c878646-67e1-485f-eb0b-18362518fa5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        }
      ],
      "source": [
        "dims = [x.shape[-1], 500, 500, 2000, 10]\n",
        "\n",
        "pretrain_epochs = 500\n",
        "batch_size = 256\n",
        "save_dir = '/content/clustering/MyDrive/clustering'\n",
        "landa = 0.1\n",
        "a_b=[1.93, 0.79]\n",
        "beta = 2\n",
        "alpha = 1\n",
        "init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
        "                           distribution='uniform')\n",
        "pretrain_optimizer = SGD(lr=1, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVg5gsE35U7j",
        "outputId": "d941a6f7-a115-4fff-fe01-819e0c484e09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/initializers/initializers.py:120: UserWarning: The initializer VarianceScaling is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Build model\n",
        "autoencoder,encoder = get_model(dims, init=init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YpEvoEe5U7s"
      },
      "source": [
        "## Pretrain auto-encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DntiCJw85Ga8"
      },
      "outputs": [],
      "source": [
        "def generate_data_generator_for_two_images(x_reliablei,Nei1_reliablei,Nei2_reliablei,batch_size):\n",
        "    genX1 = datagen.flow(x_reliablei, shuffle=False, batch_size= batch_size)\n",
        "    genX2 = datagen.flow(Nei1_reliablei, shuffle=False, batch_size= batch_size)\n",
        "    genX3 = datagen.flow(Nei2_reliablei, shuffle=False, batch_size= batch_size)   \n",
        "    X1i = genX1.next()\n",
        "    X2i = genX2 .next()\n",
        "    X3i = genX3 .next()  \n",
        "    return X1i, X2i , X3i"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer=pretrain_optimizer, loss='mse')"
      ],
      "metadata": {
        "id": "SWIW4gJ5yV32"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, rotation_range=10, zoom_range=0.0)"
      ],
      "metadata": {
        "id": "xrgF1Z4LyP6F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9A2De4LQXVT"
      },
      "outputs": [],
      "source": [
        "maxiter = x.shape[0] * 500 // batch_size\n",
        "index_array = np.arange(x.shape[0])\n",
        "loss = 0\n",
        "index = 0\n",
        "\n",
        "for ite in range(int(maxiter)):\n",
        "    idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
        "\n",
        "    # data agumentation\n",
        "    x_reshape = np.reshape(x[idx],(len(x[idx]),img_size[0],img_size[1],1))\n",
        "    datagen.fit(x_reshape)\n",
        "\n",
        "    x = np.reshape(x,(len(x),img_size[0],img_size[1],1))\n",
        "\n",
        "    X1, X2, X3, = generate_data_generator_for_two_images (x[idx],x[idx],x[idx],batch_size)\n",
        "    x = np.reshape(x,(len(x),img_size[0]*img_size[1]*1))\n",
        "    X1 = np.reshape(X1,(len(X1),img_size[0]*img_size[1]*1))\n",
        "    X2 = np.reshape(X2,(len(X2),img_size[0]*img_size[1]*1))\n",
        "    X3 = np.reshape(X3,(len(X3),img_size[0]*img_size[1]*1))\n",
        "    loss = autoencoder.train_on_batch(x=[X1, X2, X3], y=[X1])\n",
        "\n",
        "    if (index + 1) * batch_size <= x.shape[0]:\n",
        "      index = index + 1  \n",
        "    else: \n",
        "      index = 0\n",
        "    if ite%100==0:\n",
        "      print('iter = ',ite, 'loss = ',loss)\n",
        "      autoencoder.save_weights('/content/clustering/MyDrive/clustering/ae_augmentation_weights_mnist_test_unet_FC_half_new_model.h5')     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-SJ246u5U7x"
      },
      "source": [
        "### Load the pre-trained auto encoder weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KGHJeIJ1-7Zy"
      },
      "outputs": [],
      "source": [
        "Weights = [layer.get_weights() for layer in autoencoder.layers] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2GP-zLc5U70"
      },
      "source": [
        "## Build clustering model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drBo2UY55U70"
      },
      "source": [
        "### ClusteringLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BW6KaThV5U70"
      },
      "outputs": [],
      "source": [
        "class ClusteringLayer(Layer):\n",
        "    \"\"\"\n",
        "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
        "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
        "\n",
        "    # Example\n",
        "    ```\n",
        "        model.add(ClusteringLayer(n_clusters=10))\n",
        "    ```\n",
        "    # Arguments\n",
        "        n_clusters: number of clusters.\n",
        "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
        "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
        "    # Input shape\n",
        "        2D tensor with shape: `(n_samples, n_features)`.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters, FeatureWeight, weights=None, a=1.0, b=1.0, **kwargs):\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
        "        super(ClusteringLayer, self).__init__(**kwargs)\n",
        "        self.n_clusters = n_clusters\n",
        "        self.FeatureWeight = FeatureWeight\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        self.initial_weights = weights\n",
        "        self.input_spec = InputSpec(ndim=2)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 2\n",
        "        input_dim = input_shape[1]\n",
        "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
        "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
        "        if self.initial_weights is not None:\n",
        "            self.set_weights(self.initial_weights)\n",
        "            del self.initial_weights\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
        "         Measure the similarity between embedded point z_i and centroid µ_j.\n",
        "                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
        "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
        "                 (i.e., a soft assignment)\n",
        "        Arguments:\n",
        "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
        "        Return:\n",
        "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
        "        \"\"\"\n",
        "        \n",
        "        global FeatureWeight\n",
        "        q = (1 + self.a * (K.sum((self.FeatureWeight) * K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) ** (self.b))) ** (-1)  \n",
        "        FeatureWeight = K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters) * K.expand_dims(q**2, axis=2)  ,axis=0)\n",
        "        FeatureWeight = K.transpose(K.transpose(FeatureWeight) / K.sum(FeatureWeight, axis=1))                  \n",
        "        return q \n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert input_shape and len(input_shape) == 2\n",
        "        return input_shape[0], self.n_clusters\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'n_clusters': self.n_clusters}\n",
        "        base_config = super(ClusteringLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ysIy5qkx5U8G"
      },
      "outputs": [],
      "source": [
        "# computing an auxiliary target distribution\n",
        "def target_distribution(q):\n",
        "    weight = q ** 2 / q.sum(0)\n",
        "    return (weight.T / weight.sum(1)).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Xy0iRXl86K0B"
      },
      "outputs": [],
      "source": [
        "loss = 0\n",
        "index = 0\n",
        "maxiter = 8000\n",
        "index_array = np.arange(x.shape[0])\n",
        "tol = 0.0001 # tolerance threshold to stop training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlAtJgm75U98"
      },
      "source": [
        "## Model to train clustering and autoencoder at same time(Fully connected)\n",
        "Multiple outputs model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY96Sobt5U-C"
      },
      "outputs": [],
      "source": [
        "initial = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform')\n",
        "pretrain_optimizer = SGD(lr=1, momentum=0.9)\n",
        "pretrain_epochs = 300\n",
        "save_dir = '/content/clustering/MyDrive/clustering'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R5u-EdTwpQaJ"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Uklo-PmZvR7S"
      },
      "outputs": [],
      "source": [
        "FeatureWeight = np.ones((n_clusters,dims[-1]),dtype='float32')/dims[-1]\n",
        "clustering_layer = ClusteringLayer(n_clusters,FeatureWeight, a=a_b[0], b=a_b[1], name='clustering')(encoder.output)\n",
        "model = keras.Model(inputs=encoder.input, outputs=[clustering_layer, autoencoder.output])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAHfN1X25U-W"
      },
      "source": [
        "### Initialize cluster centers using k-means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Q4_qxZLQi5dT"
      },
      "outputs": [],
      "source": [
        "x = np.reshape(x,(len(x),img_size[0]*img_size[1]*1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ksjaYN55U-W"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=n_clusters, n_init=150, init='k-means++')\n",
        "y_pred = kmeans.fit_predict(encoder.predict([x,x,x]))\n",
        "model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
        "y_pred_last = np.copy(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cTjH7justXM5"
      },
      "outputs": [],
      "source": [
        "b1= 0.8\n",
        "b2 = b1/2\n",
        "\n",
        "beta_dley_rate =  (0.1*10)/(0.8*10)\n",
        "\n",
        "Prev_Number_Unreliable = 1000000000000000\n",
        "Prev_Number_reliable = 0\n",
        "\n",
        "tre_Reconstraction = 1\n",
        "tre_clustering = 0.1 \n",
        "loss_dley_rate = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kQIQ4LAZiIpf"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=['CategoricalCrossentropy', 'mse'], loss_weights=[tre_clustering, tre_Reconstraction], optimizer=pretrain_optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dYTMCinUIZOD"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, rotation_range=10, zoom_range=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "UsvwGexlHQWc"
      },
      "outputs": [],
      "source": [
        "# computing Feature weighting\n",
        "def FeatureWeight_calculator (q , alpha, x, beta, landa):\n",
        "\n",
        "  q, _  = model.predict([x,x,x],verbose=0)\n",
        "\n",
        "  mu = model.get_layer(name='clustering').get_weights()\n",
        "  mu = np.reshape(mu,(n_clusters,dims[-1]))\n",
        "\n",
        "  extract = Model(inputs = model.input, outputs = model.get_layer('add_inputs').output)\n",
        "  z = extract.predict([x, x, x])\n",
        "\n",
        "  FeatureVar = landa/np.var(z,axis=0)\n",
        "  dWkm=np.zeros((n_clusters,dims[-1]))\n",
        "\n",
        "  for j in range (n_clusters):\n",
        "    dWkm[j] = np.dot(np.transpose(q[:,j]**2) , K.square(z - mu[j]))\n",
        "  \n",
        "  tmp1=np.zeros((n_clusters,dims[-1]))\n",
        "  for j in range (dims[-1]):\n",
        "    tmp2 = (dWkm / K.expand_dims(dWkm[:,j],axis=1))**(1/(beta-1))\n",
        "    tmp1=tmp1+tmp2\n",
        "\n",
        "  FeatureWeight = np.array(1/tmp1, dtype = 'float32')\n",
        "  \n",
        "  return FeatureWeight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "vojbimuHzOw1"
      },
      "outputs": [],
      "source": [
        "del Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABr1S_Fz5U-Z"
      },
      "outputs": [],
      "source": [
        "for ite in range(int(maxiter)):\n",
        "    if ite % 140 == 0:\n",
        "        model.save_weights(save_dir + '/final_mnisttest_unet_FC_new_model.h5')\n",
        "\n",
        "        q, _  = model.predict([x,x,x],verbose=0)\n",
        "            \n",
        "        # FeatureWeight = FeatureWeight_calculator (q , alpha, x, beta, landa)\n",
        "\n",
        "        p = target_distribution(q)  # update the auxiliary target distribution p\n",
        "               \n",
        "        reliableindex, _ = np.unique(np.unique(np.where(np.sort(q,axis=1)[:,-1] - np.sort(q,axis=1)[:,-2]>=b2), np.where(np.max(q,axis=1)>=b1)),\n",
        "                           np.unique(np.where(np.sort(p,axis=1)[:,-1] - np.sort(p,axis=1)[:,-2]>=b2), np.where(np.max(p,axis=1)>=b1)))    \n",
        "\n",
        "        print('Number of reliable samples:', len(q[reliableindex]) )\n",
        "        print('Number of unreliable samples:', len(q)-len(q[reliableindex]) )\n",
        "        \n",
        "        Number_Unreliable = len(q)-len(q[reliableindex])\n",
        "        Number_reliable = len(q[reliableindex])\n",
        "\n",
        "        if (Number_reliable < Prev_Number_reliable) and ite!=0 and (0<=b1<=1) and (0<=b2<=1) and Number_reliable!=x.shape[0]:\n",
        "            b1 = b1-beta_dley_rate\n",
        "            b2 = b2-beta_dley_rate\n",
        "       \n",
        "        tre_Reconstraction = tre_Reconstraction - loss_dley_rate\n",
        "        tre_clustering = tre_clustering + loss_dley_rate\n",
        "        model.compile(loss=['CategoricalCrossentropy', 'mse'], loss_weights=[tre_clustering, tre_Reconstraction], optimizer=pretrain_optimizer)\n",
        "               \n",
        "        print('treshold of Reconstraction = ', tre_Reconstraction ,'treshold of clustering = ', tre_clustering)\n",
        "        print('Beta1 = ', b1 ,'Beta2 = ', b2)\n",
        "        Prev_Number_Unreliable = Number_Unreliable\n",
        "        Prev_Number_reliable = Number_reliable\n",
        "\n",
        "        # evaluate the clusterig performance\n",
        "        y_pred = q.argmax(1)\n",
        "        if y is not None:\n",
        "            acc = np.round(acc_fun(y, y_pred), 5)\n",
        "            nmi = np.round(nmi_fun(y, y_pred), 5)\n",
        "            ari = np.round(ari_fun(y, y_pred), 5)\n",
        "            loss = np.round(loss, 5)\n",
        "            print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
        "\n",
        "        # check stop criterion\n",
        "        delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
        "        y_pred_last = np.copy(y_pred)\n",
        "        if ite > 0 and delta_label < tol:\n",
        "            print('delta_label ', delta_label, '< tol ', tol)\n",
        "            print('Reached tolerance threshold. Stopping training.')\n",
        "            break\n",
        "\n",
        "    idx = index_array[index * batch_size: min((index+1) * batch_size, reliableindex.shape[0])]\n",
        "\n",
        "    x = np.reshape(x,(len(x),img_size[0],img_size[1],1))\n",
        "\n",
        "    X1, X2, X3, = generate_data_generator_for_two_images (x[reliableindex[idx]],\n",
        "                                                             x[reliableindex[idx]],\n",
        "                                                             x[reliableindex[idx]],batch_size)\n",
        "    \n",
        "    x = np.reshape(x,(len(x),img_size[0]*img_size[1]*1))\n",
        "    X1 = np.reshape(X1,(len(X1),img_size[0]*img_size[1]*1))\n",
        "    X2 = np.reshape(X2,(len(X2),img_size[0]*img_size[1]*1))\n",
        "    X3 = np.reshape(X3,(len(X3),img_size[0]*img_size[1]*1))\n",
        "   \n",
        "    loss = model.train_on_batch(x=[X1, X2, X3], y=[p[reliableindex[idx]], X1])    \n",
        "    index = index + 1 if (index + 1) * batch_size <= reliableindex.shape[0] else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X01puSU-5U-b"
      },
      "source": [
        "### Load the clustering model trained weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBgZ4ayY5U-d"
      },
      "source": [
        "### Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "VfavCXz4yB-U"
      },
      "outputs": [],
      "source": [
        "x = np.reshape(x,(len(x),img_size[0]*img_size[1]*1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNbKo7RPlrL7"
      },
      "outputs": [],
      "source": [
        "FeatureWeight = FeatureWeight_calculator (q , alpha, x, beta, landa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OusGcxb38ZvL"
      },
      "outputs": [],
      "source": [
        "extract = Model(inputs = model.input, outputs = model.get_layer('add_inputs').output)\n",
        "\n",
        "z = extract.predict([x,x,x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6jl31ZN3XDN"
      },
      "outputs": [],
      "source": [
        "pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTbFBY8b3G0e"
      },
      "outputs": [],
      "source": [
        "import umap.umap_ as umap\n",
        "from sklearn.datasets import fetch_openml\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(context=\"paper\", style=\"white\")\n",
        "\n",
        "reducer = umap.UMAP(random_state=42)\n",
        "embedding = reducer.fit_transform(z)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "color = y.astype(int)\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1], c=color, cmap=\"Spectral\", s=0.1)\n",
        "plt.setp(ax, xticks=[], yticks=[])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIm8WBLxlQoC"
      },
      "outputs": [],
      "source": [
        "# Eval.\n",
        "q, _  = model.predict([x,x,x], verbose=0)\n",
        "p = target_distribution(q)  # update the auxiliary target distribution p\n",
        "\n",
        "# evaluate the clustering performance\n",
        "y_pred = q.argmax(1)\n",
        "if y is not None:\n",
        "    acc = np.round(acc_fun(y, y_pred), 5)\n",
        "    nmi = np.round(nmi_fun(y, y_pred), 5)\n",
        "    ari = np.round(ari_fun(y, y_pred), 5)\n",
        "    loss = np.round(loss, 5)\n",
        "    print('Acc = %.5f, nmi = %.5f, ari = %.5f' % (acc, nmi, ari), ' ; loss=', loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8cCYcyi5U-h"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import sklearn.metrics\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set(font_scale=3)\n",
        "confusion_matrix = sklearn.metrics.confusion_matrix(y, y_pred)\n",
        "\n",
        "plt.figure(figsize=(16, 14))\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", annot_kws={\"size\": 20});\n",
        "plt.title(\"Confusion matrix\", fontsize=30)\n",
        "plt.ylabel('True label', fontsize=25)\n",
        "plt.xlabel('Clustering label', fontsize=25)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWDXlTeRU0ew"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import sklearn.metrics\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set(font_scale=3)\n",
        "confusion_matrix = sklearn.metrics.confusion_matrix(y, y_pred)\n",
        "\n",
        "plt.figure(figsize=(16, 14))\n",
        "sns.heatmap(FeatureWeight, annot=True, annot_kws={\"size\": 15});\n",
        "# plt.title(\"Feature Weight\", fontsize=30)\n",
        "plt.ylabel('Clusters', fontsize=25)\n",
        "plt.xlabel('Features', fontsize=25)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}